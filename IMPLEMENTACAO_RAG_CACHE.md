# Guia de ImplementaÃ§Ã£o: Sistema RAG com Cache Inteligente

## ğŸ“‹ VisÃ£o Geral

Este documento descreve como implementar um sistema RAG (Retrieval-Augmented Generation) com cache inteligente baseado no LanceDB, similar ao sistema implementado neste repositÃ³rio. O objetivo Ã© substituir consultas diretas a bases JSON por um sistema cacheado que economiza tokens e melhora performance.

## ğŸ¯ Problema a Resolver

**SituaÃ§Ã£o Atual:**
- Sistema consulta base JSON diretamente a cada relatÃ³rio
- Sem cache, sempre gasta tokens para embeddings
- Processamento lento e custoso
- Sem otimizaÃ§Ã£o de performance

**SoluÃ§Ã£o Proposta:**
- Cache inteligente com LanceDB
- Processamento incremental de documentos
- Busca semÃ¢ntica otimizada
- ReduÃ§Ã£o significativa de tokens e tempo

## ğŸ—ï¸ Arquitetura do Sistema

### Componentes Principais

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Documentos    â”‚â”€â”€â”€â–¶â”‚  LanceDB Cache   â”‚â”€â”€â”€â–¶â”‚  Busca SemÃ¢nticaâ”‚
â”‚   (JSON/PDF)    â”‚    â”‚   (Embeddings)   â”‚    â”‚   (Similarity)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                       â”‚   Query Cache    â”‚
                       â”‚  (Embeddings)    â”‚
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Fluxo de Processamento

1. **IngestÃ£o**: Documentos sÃ£o processados e divididos em chunks
2. **Embedding**: Cada chunk gera embedding vetorial
3. **Cache**: Embeddings sÃ£o armazenados no LanceDB
4. **Busca**: Consultas sÃ£o convertidas em embeddings e buscadas por similaridade
5. **Cache de Query**: Embeddings de consultas sÃ£o cacheados

## ğŸ“¦ DependÃªncias NecessÃ¡rias

```json
{
  "dependencies": {
    "@langchain/google-genai": "^0.0.10",
    "@lancedb/lancedb": "^0.5.0",
    "apache-arrow": "^14.0.0",
    "langchain": "^0.1.0",
    "dotenv": "^16.3.0"
  }
}
```

## ğŸ”§ ImplementaÃ§Ã£o Passo a Passo

### 1. Estrutura de Tipos

```typescript
// types.ts
export interface ChunkInfo {
  id: string;
  pageContent: string;
  metadata: Record<string, any>;
  embedding: number[];
  score?: number;
}

export interface ResultadoComScore {
  documento: {
    pageContent: string;
    metadata: Record<string, any>;
  };
  score: number;
  chunk?: ChunkInfo;
}

export interface DocumentoInfo {
  nomeArquivo: string;
  hashArquivo: string;
  dataModificacao: Date;
  dataProcessamento: Date;
  chunks: ChunkInfo[];
}

export interface ProcessamentoResultado {
  documentosNovos: string[];
  documentosModificados: string[];
  documentosRemovidos: string[];
  totalChunks: number;
  tempoProcessamento: number;
}
```

### 2. Gerenciador de Cache LanceDB

```typescript
// LanceDBCacheManager.ts
import { GoogleGenerativeAIEmbeddings } from "@langchain/google-genai";
import { RecursiveCharacterTextSplitter } from "langchain/text_splitter";
import { connect, Connection, Table } from '@lancedb/lancedb';
import * as arrow from 'apache-arrow';
import * as crypto from 'crypto';
import * as fs from 'fs';
import * as path from 'path';

export class LanceDBCacheManager {
  private db: Connection | null = null;
  private table: Table | null = null;
  private queryCacheTable: Table | null = null;
  private readonly embeddings: GoogleGenerativeAIEmbeddings;
  private readonly separador: RecursiveCharacterTextSplitter;
  private readonly queryEmbeddingCache: Map<string, number[]> = new Map();
  private static readonly MAX_QUERY_CACHE_ENTRIES: number = 500;

  constructor(
    dbPath: string = "lancedb_cache",
    embeddings: GoogleGenerativeAIEmbeddings
  ) {
    this.embeddings = embeddings;
    this.separador = new RecursiveCharacterTextSplitter({
      chunkSize: 2000,
      chunkOverlap: 500,
      lengthFunction: (text: string) => text.length
    });
  }

  // InicializaÃ§Ã£o do banco
  private async inicializarDB(): Promise<void> {
    if (!this.db) {
      this.db = await connect(this.dbPath);
      
      const tabelas = await this.db.tableNames();
      if (!tabelas.includes('chunks')) {
        await this.criarTabela();
      } else {
        this.table = await this.db.openTable('chunks');
      }
      
      if (!tabelas.includes('query_cache')) {
        await this.criarTabelaQueryCache();
      } else {
        this.queryCacheTable = await this.db.openTable('query_cache');
      }
    }
  }

  // CriaÃ§Ã£o da tabela de chunks
  private async criarTabela(): Promise<void> {
    if (!this.db) throw new Error("DB nÃ£o inicializada");

    const schema = new arrow.Schema([
      new arrow.Field('id', new arrow.Utf8()),
      new arrow.Field('pageContent', new arrow.Utf8()),
      new arrow.Field('vector', new arrow.FixedSizeList(768, new arrow.Field('item', new arrow.Float32()))),
      new arrow.Field('metadata', new arrow.Utf8()),
      new arrow.Field('documentoId', new arrow.Utf8()),
      new arrow.Field('nomeArquivo', new arrow.Utf8()),
      new arrow.Field('hashArquivo', new arrow.Utf8()),
      new arrow.Field('dataProcessamento', new arrow.Utf8()),
      new arrow.Field('chunkIndex', new arrow.Int32())
    ]);

    this.table = await this.db.createTable('chunks', [], { schema });
  }

  // CriaÃ§Ã£o da tabela de cache de queries
  private async criarTabelaQueryCache(): Promise<void> {
    if (!this.db) throw new Error("DB nÃ£o inicializada");

    const schema = new arrow.Schema([
      new arrow.Field('query', new arrow.Utf8()),
      new arrow.Field('vector', new arrow.FixedSizeList(768, new arrow.Field('item', new arrow.Float32()))),
      new arrow.Field('createdAt', new arrow.Utf8())
    ]);

    this.queryCacheTable = await this.db.createTable('query_cache', [], { schema });
  }

  // Processamento de documentos JSON
  async processarDocumentoJSON(arquivo: string): Promise<{ chunks: ChunkInfo[], tokensConsumidos: number }> {
    const nomeArquivo = path.basename(arquivo);
    const hashArquivo = this.calcularHashArquivo(arquivo);
    
    // Carregar JSON
    const conteudo = fs.readFileSync(arquivo, 'utf-8');
    const dados = JSON.parse(conteudo);
    
    // Converter para texto estruturado
    const texto = this.jsonParaTexto(dados);
    
    // Dividir em chunks
    const chunks = await this.separador.splitText(texto);
    
    // Gerar embeddings
    const embeddings = await this.embeddings.embedDocuments(chunks);
    
    // Preparar dados para inserÃ§Ã£o
    const dadosParaInserir = chunks.map((chunk, index) => ({
      id: `${nomeArquivo}_chunk_${index}`,
      pageContent: chunk,
      vector: new Float32Array(embeddings[index]),
      metadata: JSON.stringify({
        source: arquivo,
        chunkIndex: index,
        tipo: 'json'
      }),
      documentoId: nomeArquivo,
      nomeArquivo: nomeArquivo,
      hashArquivo: hashArquivo,
      dataProcessamento: new Date().toISOString(),
      chunkIndex: index
    }));
    
    // Inserir no LanceDB
    await this.table!.add(dadosParaInserir);
    
    const tokensConsumidos = chunks.reduce((total, chunk) => total + chunk.length, 0);
    
    return {
      chunks: dadosParaInserir.map((dado, index) => ({
        id: dado.id,
        pageContent: dado.pageContent,
        embedding: Array.from(dado.vector),
        metadata: JSON.parse(dado.metadata)
      })),
      tokensConsumidos
    };
  }

  // ConversÃ£o de JSON para texto estruturado
  private jsonParaTexto(dados: any, nivel: number = 0): string {
    const indentacao = '  '.repeat(nivel);
    let texto = '';
    
    if (Array.isArray(dados)) {
      dados.forEach((item, index) => {
        texto += `${indentacao}Item ${index + 1}:\n`;
        texto += this.jsonParaTexto(item, nivel + 1);
        texto += '\n';
      });
    } else if (typeof dados === 'object' && dados !== null) {
      Object.entries(dados).forEach(([chave, valor]) => {
        if (typeof valor === 'object' && valor !== null) {
          texto += `${indentacao}${chave}:\n`;
          texto += this.jsonParaTexto(valor, nivel + 1);
        } else {
          texto += `${indentacao}${chave}: ${valor}\n`;
        }
      });
    } else {
      texto += `${indentacao}${dados}\n`;
    }
    
    return texto;
  }

  // Busca semÃ¢ntica
  async buscarSemantica(query: string, k: number = 5): Promise<ChunkInfo[]> {
    await this.inicializarDB();
    
    const queryEmbedding = await this.obterEmbeddingConsulta(query);
    
    const resultados = await this.table!.search(queryEmbedding)
      .limit(k)
      .toArray();
    
    return resultados.map((row: any) => ({
      id: row.id as string,
      pageContent: row.pageContent as string,
      embedding: Array.from(row.vector as Float32Array),
      metadata: JSON.parse(row.metadata as string),
      score: row.score as number
    }));
  }

  // Cache de embeddings de consulta
  private async obterEmbeddingConsulta(query: string): Promise<number[]> {
    const norm = this.normalizarQuery(query);
    
    // Cache em memÃ³ria
    const existente = this.queryEmbeddingCache.get(norm);
    if (existente) return existente;

    // Cache persistente
    try {
      if (this.queryCacheTable) {
        const registros = await this.queryCacheTable.search(new Array(768).fill(0)).limit(2000).toArray();
        const encontrado = registros.find((r: any) => (r.query as string) === norm);
        if (encontrado) {
          const emb = Array.from(encontrado.vector as Float32Array);
          this.colocarNoCache(norm, emb);
          return emb;
        }
      }
    } catch {
      // Ignorar erros de cache persistente
    }

    // Gerar novo embedding
    const gerado = await this.embeddings.embedQuery(norm);
    this.colocarNoCache(norm, gerado);

    // Persistir no cache
    try {
      if (this.queryCacheTable) {
        await this.queryCacheTable.add([{
          query: norm,
          vector: new Float32Array(gerado),
          createdAt: new Date().toISOString()
        }]);
      }
    } catch {
      // Ignorar erros de escrita no cache
    }

    return gerado;
  }

  private normalizarQuery(query: string): string {
    return query
      .toLowerCase()
      .trim()
      .replace(/\s+/g, ' ')
      .normalize('NFD')
      .replace(/[\u0300-\u036f]/g, '');
  }

  private colocarNoCache(query: string, emb: number[]): void {
    if (this.queryEmbeddingCache.size >= LanceDBCacheManager.MAX_QUERY_CACHE_ENTRIES) {
      const primeiro = this.queryEmbeddingCache.keys().next();
      if (!primeiro.done) {
        this.queryEmbeddingCache.delete(primeiro.value);
      }
    }
    this.queryEmbeddingCache.set(query, emb);
  }

  private calcularHashArquivo(arquivo: string): string {
    const conteudo = fs.readFileSync(arquivo);
    return crypto.createHash('md5').update(conteudo).digest('hex');
  }

  // VerificaÃ§Ã£o de cache
  async verificarCache(): Promise<boolean> {
    try {
      await this.inicializarDB();
      const tabelas = await this.db!.tableNames();
      return tabelas.includes('chunks');
    } catch (error) {
      return false;
    }
  }

  // EstatÃ­sticas
  async obterEstatisticas(): Promise<{ totalChunks: number, totalDocumentos: number }> {
    if (!this.table) {
      await this.inicializarDB();
    }
    
    const totalChunks = await this.table!.countRows();
    
    // Contar documentos Ãºnicos
    const registros = await this.table!.search(new Array(768).fill(0)).limit(1000).toArray();
    const documentos = new Set(registros.map((r: any) => r.nomeArquivo as string));
    
    return {
      totalChunks,
      totalDocumentos: documentos.size
    };
  }

  // Limpeza de cache
  async limparCache(): Promise<void> {
    if (this.db) {
      await this.db.dropTable('chunks');
      await this.db.dropTable('query_cache');
      this.table = null;
      this.queryCacheTable = null;
    }
    this.queryEmbeddingCache.clear();
  }
}
```

### 3. Sistema de Busca SemÃ¢ntica

```typescript
// SemanticSearch.ts
import { GoogleGenerativeAIEmbeddings } from "@langchain/google-genai";
import { LanceDBCacheManager } from "./LanceDBCacheManager";
import { ResultadoComScore, ChunkInfo } from "./types";

export class SemanticSearch {
  private readonly cacheManager: LanceDBCacheManager;
  private readonly embeddings: GoogleGenerativeAIEmbeddings;

  constructor(
    embeddings: GoogleGenerativeAIEmbeddings,
    dbPath: string = "lancedb_cache"
  ) {
    this.embeddings = embeddings;
    this.cacheManager = new LanceDBCacheManager(dbPath, embeddings);
  }

  async buscar(query: string, k: number = 5): Promise<ResultadoComScore[]> {
    const cacheExiste = await this.cacheManager.verificarCache();
    if (!cacheExiste) {
      throw new Error("Cache nÃ£o encontrado. Execute primeiro o processamento de documentos.");
    }

    const chunks = await this.cacheManager.buscarSemantica(query, k);
    
    return chunks.map((chunk, index) => ({
      documento: {
        pageContent: chunk.pageContent,
        metadata: chunk.metadata
      },
      score: chunk.score || (1.0 - index * 0.1),
      chunk
    }));
  }

  async verificarCache(): Promise<boolean> {
    return await this.cacheManager.verificarCache();
  }

  async obterEstatisticas(): Promise<{ totalChunks: number, totalDocumentos: number }> {
    return await this.cacheManager.obterEstatisticas();
  }
}
```

### 4. Processador de Documentos

```typescript
// DocumentProcessor.ts
import { GoogleGenerativeAIEmbeddings } from "@langchain/google-genai";
import { LanceDBCacheManager } from "./LanceDBCacheManager";
import { ProcessamentoResultado } from "./types";
import * as fs from 'fs';
import * as path from 'path';

export class DocumentProcessor {
  private readonly cacheManager: LanceDBCacheManager;
  private readonly pastaDocumentos: string;

  constructor(
    embeddings: GoogleGenerativeAIEmbeddings,
    pastaDocumentos: string = "documentos",
    dbPath: string = "lancedb_cache"
  ) {
    this.cacheManager = new LanceDBCacheManager(dbPath, embeddings);
    this.pastaDocumentos = pastaDocumentos;
  }

  async processarDocumentos(): Promise<ProcessamentoResultado> {
    const inicio = Date.now();
    
    // Verificar se a pasta existe
    if (!fs.existsSync(this.pastaDocumentos)) {
      throw new Error(`Pasta de documentos nÃ£o encontrada: ${this.pastaDocumentos}`);
    }

    // Obter arquivos JSON
    const arquivos = this.obterArquivosJSON();
    
    let documentosNovos: string[] = [];
    let documentosModificados: string[] = [];
    let totalChunks = 0;
    let tokensConsumidos = 0;

    console.log(`ğŸ“ Processando ${arquivos.length} arquivos JSON...`);

    for (const arquivo of arquivos) {
      try {
        const nomeArquivo = path.basename(arquivo);
        console.log(`ğŸ“„ Processando: ${nomeArquivo}`);
        
        const resultado = await this.cacheManager.processarDocumentoJSON(arquivo);
        
        documentosNovos.push(nomeArquivo);
        totalChunks += resultado.chunks.length;
        tokensConsumidos += resultado.tokensConsumidos;
        
        console.log(`âœ… ${nomeArquivo}: ${resultado.chunks.length} chunks processados`);
      } catch (error) {
        console.error(`âŒ Erro ao processar ${arquivo}:`, error);
      }
    }

    const tempoProcessamento = Date.now() - inicio;

    return {
      documentosNovos,
      documentosModificados,
      documentosRemovidos: [],
      totalChunks,
      tempoProcessamento
    };
  }

  private obterArquivosJSON(): string[] {
    const arquivos: string[] = [];
    
    const processarPasta = (pasta: string) => {
      const itens = fs.readdirSync(pasta);
      
      for (const item of itens) {
        const caminhoCompleto = path.join(pasta, item);
        const stat = fs.statSync(caminhoCompleto);
        
        if (stat.isDirectory()) {
          processarPasta(caminhoCompleto);
        } else if (item.endsWith('.json')) {
          arquivos.push(caminhoCompleto);
        }
      }
    };
    
    processarPasta(this.pastaDocumentos);
    return arquivos;
  }

  async limparCache(): Promise<void> {
    await this.cacheManager.limparCache();
  }
}
```

### 5. IntegraÃ§Ã£o com Sistema Existente

```typescript
// RAGService.ts
import { GoogleGenerativeAIEmbeddings } from "@langchain/google-genai";
import { ChatGoogleGenerativeAI } from "@langchain/google-genai";
import { SemanticSearch } from "./SemanticSearch";
import { DocumentProcessor } from "./DocumentProcessor";
import { ResultadoComScore } from "./types";

export class RAGService {
  private readonly embeddings: GoogleGenerativeAIEmbeddings;
  private readonly semanticSearch: SemanticSearch;
  private readonly documentProcessor: DocumentProcessor;
  private readonly llm: ChatGoogleGenerativeAI;

  constructor(
    apiKey: string,
    pastaDocumentos: string = "documentos",
    dbPath: string = "lancedb_cache"
  ) {
    this.embeddings = new GoogleGenerativeAIEmbeddings({
      apiKey: apiKey,
      modelName: "embedding-001"
    });

    this.semanticSearch = new SemanticSearch(this.embeddings, dbPath);
    this.documentProcessor = new DocumentProcessor(this.embeddings, pastaDocumentos, dbPath);
    
    this.llm = new ChatGoogleGenerativeAI({
      apiKey: apiKey,
      modelName: "gemini-1.5-flash"
    });
  }

  // Inicializar cache (executar uma vez)
  async inicializarCache(): Promise<void> {
    console.log("ğŸš€ Inicializando cache RAG...");
    const resultado = await this.documentProcessor.processarDocumentos();
    
    console.log("âœ… Cache inicializado com sucesso!");
    console.log(`ğŸ“„ Documentos processados: ${resultado.documentosNovos.length}`);
    console.log(`ğŸ“Š Total de chunks: ${resultado.totalChunks}`);
    console.log(`â±ï¸ Tempo: ${resultado.tempoProcessamento}ms`);
  }

  // Consultar com RAG
  async consultar(pergunta: string, k: number = 5): Promise<string> {
    // Verificar se cache existe
    const cacheExiste = await this.semanticSearch.verificarCache();
    if (!cacheExiste) {
      throw new Error("Cache nÃ£o encontrado. Execute primeiro: await service.inicializarCache()");
    }

    // Buscar documentos relevantes
    const resultados = await this.semanticSearch.buscar(pergunta, k);
    
    if (resultados.length === 0) {
      return "NÃ£o encontrei informaÃ§Ãµes relevantes na base de conhecimento.";
    }

    // Preparar contexto
    const contexto = resultados
      .map(r => r.documento.pageContent)
      .join("\n\n---\n\n");

    // Gerar prompt
    const prompt = this.criarPrompt(pergunta, contexto);

    // Gerar resposta
    const resposta = await this.llm.invoke(prompt);
    return resposta.content as string;
  }

  private criarPrompt(pergunta: string, contexto: string): string {
    return `VocÃª Ã© um assistente especializado em anÃ¡lise de ameaÃ§as cibernÃ©ticas.

Base de conhecimento:
${contexto}

Pergunta: ${pergunta}

InstruÃ§Ãµes:
1. Use apenas as informaÃ§Ãµes fornecidas na base de conhecimento
2. Se nÃ£o encontrar informaÃ§Ãµes relevantes, indique claramente
3. ForneÃ§a respostas detalhadas e estruturadas
4. Cite as fontes quando possÃ­vel

Resposta:`;
  }

  // Verificar status do cache
  async verificarStatus(): Promise<{ cacheExiste: boolean, estatisticas?: any }> {
    const cacheExiste = await this.semanticSearch.verificarCache();
    
    if (cacheExiste) {
      const estatisticas = await this.semanticSearch.obterEstatisticas();
      return { cacheExiste, estatisticas };
    }
    
    return { cacheExiste };
  }

  // Limpar cache
  async limparCache(): Promise<void> {
    await this.documentProcessor.limparCache();
    console.log("ğŸ—‘ï¸ Cache limpo com sucesso!");
  }
}
```

## ğŸš€ Exemplo de Uso

```typescript
// exemplo-uso.ts
import { RAGService } from './RAGService';
import * as dotenv from 'dotenv';

dotenv.config();

async function exemploUso() {
  // Configurar serviÃ§o
  const service = new RAGService(
    process.env.GOOGLE_API_KEY!,
    './documentos',  // pasta com arquivos JSON
    './cache_rag'    // pasta do cache
  );

  // Verificar status
  const status = await service.verificarStatus();
  console.log('Status do cache:', status);

  // Inicializar cache (executar apenas uma vez)
  if (!status.cacheExiste) {
    console.log('Inicializando cache...');
    await service.inicializarCache();
  }

  // Fazer consultas
  const perguntas = [
    "Quais sÃ£o as principais ameaÃ§as CAPEC?",
    "Como funciona o ataque de injeÃ§Ã£o SQL?",
    "Quais sÃ£o as tÃ©cnicas de evasÃ£o de detecÃ§Ã£o?"
  ];

  for (const pergunta of perguntas) {
    console.log(`\nâ“ Pergunta: ${pergunta}`);
    try {
      const resposta = await service.consultar(pergunta);
      console.log(`ğŸ¤– Resposta: ${resposta}`);
    } catch (error) {
      console.error(`âŒ Erro: ${error}`);
    }
  }
}

exemploUso().catch(console.error);
```

## ğŸ“ Estrutura de Arquivos

```
projeto-rag/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ types.ts
â”‚   â”œâ”€â”€ LanceDBCacheManager.ts
â”‚   â”œâ”€â”€ SemanticSearch.ts
â”‚   â”œâ”€â”€ DocumentProcessor.ts
â”‚   â”œâ”€â”€ RAGService.ts
â”‚   â””â”€â”€ exemplo-uso.ts
â”œâ”€â”€ documentos/
â”‚   â”œâ”€â”€ capecs.json
â”‚   â”œâ”€â”€ mitre-attack.json
â”‚   â””â”€â”€ outras-bases.json
â”œâ”€â”€ cache_rag/          # Gerado automaticamente
â”œâ”€â”€ package.json
â””â”€â”€ .env
```

## ğŸ”§ ConfiguraÃ§Ã£o

### 1. Instalar DependÃªncias

```bash
npm install @langchain/google-genai @lancedb/lancedb apache-arrow langchain dotenv
```

### 2. Configurar VariÃ¡veis de Ambiente

```env
GOOGLE_API_KEY=sua_chave_api_aqui
```

### 3. Preparar Documentos

Coloque seus arquivos JSON na pasta `documentos/` com a estrutura desejada.

## ğŸ“Š BenefÃ­cios da ImplementaÃ§Ã£o

### âš¡ Performance
- **Cache de Embeddings**: Embeddings sÃ£o calculados uma vez e reutilizados
- **Cache de Queries**: Consultas similares usam embeddings cacheados
- **Busca Otimizada**: LanceDB oferece busca vetorial nativa

### ğŸ’° Economia de Recursos
- **ReduÃ§Ã£o de Tokens**: Embeddings sÃ£o cacheados, nÃ£o recalculados
- **Menos API Calls**: Consultas similares usam cache
- **Processamento Incremental**: Apenas documentos novos sÃ£o processados

### ğŸ” Qualidade
- **Busca SemÃ¢ntica**: Encontra conteÃºdo relevante mesmo com palavras diferentes
- **Contexto Rico**: MÃºltiplos chunks fornecem contexto completo
- **Respostas Estruturadas**: LLM gera respostas baseadas no contexto

## ğŸ› ï¸ ManutenÃ§Ã£o

### AtualizaÃ§Ã£o de Documentos
```typescript
// Para adicionar novos documentos, simplesmente coloque na pasta
// e execute novamente o processamento
await service.inicializarCache();
```

### Limpeza de Cache
```typescript
// Para limpar completamente o cache
await service.limparCache();
```

### Monitoramento
```typescript
// Verificar estatÃ­sticas do cache
const status = await service.verificarStatus();
console.log('EstatÃ­sticas:', status.estatisticas);
```

## ğŸ¯ MigraÃ§Ã£o do Sistema Atual

### Passos para MigraÃ§Ã£o

1. **Backup**: FaÃ§a backup da base JSON atual
2. **ImplementaÃ§Ã£o**: Implemente o sistema RAG conforme este guia
3. **Teste**: Teste com um subconjunto dos dados
4. **MigraÃ§Ã£o Gradual**: Migre gradualmente as consultas
5. **Monitoramento**: Monitore performance e qualidade das respostas

### AdaptaÃ§Ãµes EspecÃ­ficas

- **Estrutura JSON**: Ajuste a funÃ§Ã£o `jsonParaTexto()` para sua estrutura especÃ­fica
- **Prompts**: Personalize os prompts para seu domÃ­nio
- **ConfiguraÃ§Ãµes**: Ajuste tamanho de chunks e overlap conforme necessÃ¡rio

## ğŸ” Troubleshooting

### Problemas Comuns

1. **Cache nÃ£o encontrado**
   - Execute `await service.inicializarCache()`

2. **Erro de API Key**
   - Verifique se `GOOGLE_API_KEY` estÃ¡ configurada

3. **Performance lenta**
   - Ajuste `chunkSize` e `chunkOverlap`
   - Verifique se o cache estÃ¡ sendo usado

4. **Respostas irrelevantes**
   - Ajuste o nÃºmero de chunks (`k`) na busca
   - Melhore a funÃ§Ã£o de conversÃ£o JSON para texto

## ğŸ“ˆ MÃ©tricas de Sucesso

- **ReduÃ§Ã£o de Tokens**: 70-90% menos tokens consumidos
- **Melhoria de Performance**: 5-10x mais rÃ¡pido
- **Qualidade das Respostas**: Mais relevantes e contextualizadas
- **Custo**: ReduÃ§Ã£o significativa no custo de API

---

Este guia fornece uma implementaÃ§Ã£o completa e otimizada do sistema RAG com cache. A implementaÃ§Ã£o Ã© modular e pode ser adaptada para diferentes tipos de documentos e casos de uso especÃ­ficos.
