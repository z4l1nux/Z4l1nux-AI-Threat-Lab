import neo4j, { Driver, Session, Node, Integer } from 'neo4j-driver';
import { RecursiveCharacterTextSplitter, MarkdownTextSplitter } from "@langchain/textsplitters";
import { Document } from "@langchain/core/documents";
import * as crypto from 'crypto';
import { Neo4jDocument, Neo4jChunk, Neo4jSearchResult, DocumentUpload } from '../../types/index';

export class Neo4jCacheManager {
  private driver: Driver;
  private splitter: RecursiveCharacterTextSplitter;
  private markdownSplitter: MarkdownTextSplitter;
  private embeddingCache: Map<string, number[]> = new Map();
  private ollamaUnavailableUntil: number | null = null; // circuit breaker simples

  constructor(
    neo4jUri: string,
    neo4jUser: string, 
    neo4jPassword: string
  ) {
    // Validar par√¢metros obrigat√≥rios
    if (!neo4jUri) {
      throw new Error("‚ùå Neo4j URI √© obrigat√≥rio");
    }
    if (!neo4jUser) {
      throw new Error("‚ùå Neo4j USER √© obrigat√≥rio");
    }
    if (!neo4jPassword) {
      throw new Error("‚ùå Neo4j PASSWORD √© obrigat√≥rio");
    }

    this.driver = neo4j.driver(neo4jUri, neo4j.auth.basic(neo4jUser, neo4jPassword));
    
    // Splitter padr√£o para textos gen√©ricos
    this.splitter = new RecursiveCharacterTextSplitter({
      chunkSize: 4000,
      chunkOverlap: 800,
      lengthFunction: (text: string) => text.length
    });

    // Splitter espec√≠fico para Markdown que respeita headers e estrutura
    this.markdownSplitter = new MarkdownTextSplitter({
      chunkSize: 8000,  // Maior para manter se√ß√µes completas
      chunkOverlap: 1000
    });
  }

  // M√©todo para obter labels simplificados (apenas nomic-embed-text)
  private getEmbeddingLabels(embeddingProvider: string): { documentLabel: string; chunkLabel: string } {
    // Sempre usar labels simples, independente do provedor
    return { documentLabel: 'Document', chunkLabel: 'Chunk' };
  }

  private async pingOllama(timeoutMs: number = 1000): Promise<boolean> {
    try {
      const base = process.env.OLLAMA_BASE_URL;
      if (!base) return false;
      const controller = new AbortController();
      const t = setTimeout(() => controller.abort(), timeoutMs);
      const res = await fetch(`${base}/api/tags`, { signal: controller.signal });
      clearTimeout(t);
      return res.ok;
    } catch {
      return false;
    }
  }

  // M√©todo para obter dimens√µes do embedding (apenas nomic-embed-text)
  private getEmbeddingDimensions(embeddingProvider: string): number {
    // Sempre usar 768 dimens√µes (nomic-embed-text)
    return 768;
  }

  // M√©todo para determinar o melhor provedor de embedding dispon√≠vel
  private async getBestAvailableEmbeddingProvider(requestedProvider?: string): Promise<string> {
    // Se um provedor espec√≠fico foi solicitado, verificar se est√° realmente dispon√≠vel
    if (requestedProvider) {
      if (requestedProvider === 'ollama' && process.env.OLLAMA_BASE_URL) {
        // Verificar se Ollama est√° realmente dispon√≠vel
        try {
          const response = await fetch(`${process.env.OLLAMA_BASE_URL}/api/tags`, { 
            method: 'GET',
            signal: AbortSignal.timeout(5000) // 5s timeout
          });
          if (response.ok) {
            console.log('üîÑ Usando Ollama para embeddings (solicitado e dispon√≠vel)');
            return 'ollama';
          }
        } catch (error) {
          console.warn('‚ö†Ô∏è Ollama solicitado mas n√£o dispon√≠vel, tentando fallback...');
        }
      }
      if (requestedProvider === 'gemini' && process.env.GEMINI_API_KEY) {
        console.log('üîÑ Usando Gemini para embeddings (solicitado e dispon√≠vel)');
        return 'gemini';
      }
      if (requestedProvider === 'openrouter' && (process.env.OPENAI_API_KEY || process.env.OPENROUTER_API_KEY)) {
        console.log('üîÑ Usando OpenAI para embeddings (solicitado e dispon√≠vel)');
        return 'openai'; // OpenRouter usa OpenAI para embeddings
      }
    }

    // Fallback: verificar provedores dispon√≠veis na ordem de prioridade
    // 1. Gemini (mais confi√°vel, bom custo-benef√≠cio)
    if (process.env.GEMINI_API_KEY) {
      console.log('üîÑ Usando Gemini para embeddings (fallback autom√°tico)');
      return 'gemini';
    }
    
    // 2. OpenAI (via OpenRouter config ou direto)
    if (process.env.OPENAI_API_KEY || process.env.OPENROUTER_API_KEY) {
      console.log('üîÑ Usando OpenAI para embeddings (fallback autom√°tico)');
      return 'openai';
    }
    
    // 3. Ollama (local, mais r√°pido) - s√≥ se realmente dispon√≠vel
    if (process.env.OLLAMA_BASE_URL && process.env.EMBEDDING_MODEL) {
      try {
        const response = await fetch(`${process.env.OLLAMA_BASE_URL}/api/tags`, { 
          method: 'GET',
          signal: AbortSignal.timeout(5000) // 5s timeout
        });
        if (response.ok) {
          console.log('üîÑ Usando Ollama para embeddings (fallback autom√°tico)');
          return 'ollama';
        }
      } catch (error) {
        console.warn('‚ö†Ô∏è Ollama configurado mas n√£o dispon√≠vel, pulando...');
      }
    }

    // Se nenhum dispon√≠vel, usar Gemini como √∫ltimo recurso (mais confi√°vel)
    if (process.env.GEMINI_API_KEY) {
      console.log('üîÑ Usando Gemini como √∫ltimo recurso para embeddings');
      return 'gemini';
    }

    // Se realmente nenhum dispon√≠vel, erro
    throw new Error('‚ùå Nenhum provedor de embedding dispon√≠vel. Configure pelo menos um: Gemini, OpenAI ou Ollama.');
  }

  // M√©todo para obter modelo de embedding baseado na configura√ß√£o e provider
  private getEmbeddingModel(modelConfig?: any, provider?: string): string {
    // Usar modelo do modelConfig APENAS se o provider coincidir
    if (modelConfig?.embedding && modelConfig?.embeddingProvider && provider && modelConfig.embeddingProvider === provider) {
      return modelConfig.embedding;
    }
    
    // Caso contr√°rio, usar modelo espec√≠fico do provider
    if (provider === 'gemini' && process.env.EMBEDDING_MODEL_GEMINI) {
      return process.env.EMBEDDING_MODEL_GEMINI;
    } else if ((provider === 'openrouter' || provider === 'openai') && process.env.EMBEDDING_MODEL_OPENROUTER) {
      return process.env.EMBEDDING_MODEL_OPENROUTER;
    } else if (provider === 'ollama' && process.env.EMBEDDING_MODEL) {
      return process.env.EMBEDDING_MODEL;
    }
    
    // Fallback padr√£o por provider
    if (provider === 'gemini') {
      return 'text-embedding-004';
    } else if (provider === 'openrouter' || provider === 'openai') {
      return 'text-embedding-3-small';
    } else {
      return 'nomic-embed-text:latest';
    }
  }

  /**
   * Limpar cache de embeddings
   */
  clearEmbeddingCache(): void {
    this.embeddingCache.clear();
    console.log('üßπ Cache de embeddings limpo');
  }

  /**
   * Obter estat√≠sticas do cache
   */
  getCacheStats(): { size: number; maxSize: number } {
    return {
      size: this.embeddingCache.size,
      maxSize: 100
    };
  }

  async initialize(): Promise<void> {
    const session = this.driver.session();
    
    try {
      console.log("üîß Inicializando Neo4j Cache Manager com labels por provedor...");
      
      // Criar constraints √∫nicos para cada tipo de documento
      const providers = ['Ollama', 'OpenRouter'];
      
      for (const provider of providers) {
        await session.run(`
          CREATE CONSTRAINT document_${provider.toLowerCase()}_id_unique IF NOT EXISTS
          FOR (d:Document) REQUIRE d.id IS UNIQUE
        `);
        
        await session.run(`
          CREATE CONSTRAINT chunk_${provider.toLowerCase()}_id_unique IF NOT EXISTS
          FOR (c:Chunk) REQUIRE c.id IS UNIQUE
        `);
      }

      // Evitar documentos duplicados por conte√∫do
      await session.run(`
        CREATE CONSTRAINT document_hash_unique IF NOT EXISTS
        FOR (d:Document) REQUIRE d.hash IS UNIQUE
      `);
      
            // Criar √≠ndices vetoriais para cada provedor
            // Criar √≠ndice vetorial √∫nico para nomic-embed-text (768 dimens√µes)
            try {
              await session.run(`
                CREATE VECTOR INDEX chunk_embeddings IF NOT EXISTS
                FOR (c:Chunk) ON (c.embedding)
                OPTIONS {indexConfig: {
                  \`vector.dimensions\`: 768,
                  \`vector.similarity_function\`: 'cosine'
                }}
              `);
              console.log(`‚úÖ √çndice vetorial √∫nico criado para nomic-embed-text (768 dimens√µes)`);
            } catch (error) {
              console.warn(`‚ö†Ô∏è Erro ao criar √≠ndice vetorial √∫nico:`, error);
            }
      
      console.log("‚úÖ Neo4j Cache Manager inicializado com √≠ndice √∫nico para nomic-embed-text");
      
    } finally {
      await session.close();
    }
  }

        async processDocumentFromMemory(document: DocumentUpload, modelConfig?: any): Promise<void> {
          // Usar estrutura unificada (sem labels espec√≠ficos de provedor)
          const session = this.driver.session();
    
    try {
      // Detectar provider de embedding dispon√≠vel automaticamente
      const requestedProvider = modelConfig?.embeddingProvider;
      const provider = await this.getBestAvailableEmbeddingProvider(requestedProvider);
      
      console.log(`üß† Processando documento com embedding ${provider}: ${document.name}`);
      
      // Gerar hash do conte√∫do (para detectar mudan√ßas)
      const documentHash = crypto.createHash('sha256').update(document.content).digest('hex');
      
      // Usar hash baseado no nome para ID consistente
      const documentId = crypto.createHash('md5').update(document.name).digest('hex');
      
      // Verificar se documento j√° existe pelo ID (nome) ou pelo HASH (conte√∫do)
      const existingDoc = await session.run(`
        MATCH (d:Document {id: $documentId})
        RETURN d.hash as hash
      `, { documentId });

      const shouldUpdate = existingDoc.records.length > 0 && 
                          existingDoc.records[0].get('hash') !== documentHash;
      
      // Se j√° existir um documento com o mesmo conte√∫do (hash), pular reimporta√ß√£o
      const existingByHash = await session.run(`
        MATCH (d:Document {hash: $documentHash})
        RETURN d.id as id, d.name as name
      `, { documentHash });

      if (existingByHash.records.length > 0) {
        const existingId = existingByHash.records[0].get('id');
        const existingName = existingByHash.records[0].get('name');
        // Se o mesmo conte√∫do j√° estiver presente (mesmo hash), n√£o reprocessar
        if (existingId !== documentId || !shouldUpdate) {
          console.log(`‚è≠Ô∏è Documento com mesmo conte√∫do j√° existe (${existingName}). Pulando: ${document.name}`);
          return;
        }
      }

      if (existingDoc.records.length > 0) {
        if (shouldUpdate) {
          console.log(`üîÑ Documento existente encontrado, atualizando: ${document.name}`);
          // Deletar chunks antigos
        await session.run(`
          MATCH (d:Document {id: $documentId})-[:CONTAINS]->(c:Chunk)
          DETACH DELETE c
        `, { documentId });
        } else {
          console.log(`‚è≠Ô∏è Documento id√™ntico j√° existe, pulando: ${document.name}`);
          return;
        }
      }
      
      // Dividir em chunks - usar MarkdownSplitter para arquivos .md
      const isMarkdown = document.name.toLowerCase().endsWith('.md');
      const splitterToUse = isMarkdown ? this.markdownSplitter : this.splitter;
      
      if (isMarkdown) {
        console.log(`üìù Usando MarkdownTextSplitter para ${document.name} (respeita estrutura de headers)`);
      }
      
      const chunks = await splitterToUse.createDocuments([document.content]);
      
      if (chunks.length === 0) {
        throw new Error(`Nenhum chunk gerado para: ${document.name}`);
      }

      // Usar o provider j√° verificado como dispon√≠vel
      console.log(`üìÑ Gerando embeddings ${provider} para ${chunks.length} chunks...`);
      
      // Gerar embeddings para todos os chunks
      const embeddings: number[][] = [];
      for (let i = 0; i < chunks.length; i++) {
        try {
          let embedding: number[];
          
          if (provider === 'ollama') {
            console.log(`üîó Usando Ollama para embedding do chunk ${i + 1}`);
            const baseUrl = process.env.OLLAMA_BASE_URL ?? '';
            const response = await fetch(`${baseUrl}/api/embeddings`, {
              method: 'POST',
              headers: { 'Content-Type': 'application/json' },
              body: JSON.stringify({
                model: this.getEmbeddingModel(modelConfig, 'ollama'),
                prompt: chunks[i].pageContent
              })
            });
            if (!response.ok) throw new Error(`HTTP ${response.status}`);
            const data = await response.json() as { embedding: number[] };
            embedding = data.embedding;
          } else if (provider === 'gemini') {
            console.log(`üîó Usando Gemini para embedding do chunk ${i + 1}`);
            const { GoogleGenerativeAI } = await import('@google/generative-ai');
            const genAI = new GoogleGenerativeAI(process.env.GEMINI_API_KEY || '');
            const embeddingModel = genAI.getGenerativeModel({ 
              model: this.getEmbeddingModel(modelConfig, 'gemini')
            });
            
            const result = await embeddingModel.embedContent(chunks[i].pageContent);
            embedding = result.embedding.values;
          } else if (provider === 'openai') {
            // Usar OpenAI para embeddings (OpenRouter n√£o suporta embeddings)
            console.log(`üîó Usando OpenAI para embedding do chunk ${i + 1}`);
            const apiKey = process.env.OPENAI_API_KEY || process.env.OPENROUTER_API_KEY;
            
            if (!apiKey) {
              throw new Error('OPENAI_API_KEY ou OPENROUTER_API_KEY n√£o configurada');
            }
            
            try {
              const response = await fetch('https://api.openai.com/v1/embeddings', {
                method: 'POST',
                headers: {
                  'Authorization': `Bearer ${apiKey}`,
                  'Content-Type': 'application/json'
                },
                body: JSON.stringify({
                  model: this.getEmbeddingModel(modelConfig, provider),
                  input: chunks[i].pageContent
                })
              });

              if (response.ok) {
                const data = await response.json() as { data: Array<{ embedding: number[] }> };
                embedding = data.data[0].embedding;
              } else {
                const errorText = await response.text();
                console.error(`‚ùå Erro na resposta da OpenAI:`, errorText);
                throw new Error(`OpenAI embedding failed: ${response.statusText}`);
              }
            } catch (error) {
              console.warn(`‚ö†Ô∏è Erro com OpenAI embedding:`, error);
              throw new Error(`Falha ao gerar embedding com OpenAI: ${error instanceof Error ? error.message : 'Unknown error'}`);
            }
          } else {
            throw new Error(`Provider de embedding n√£o suportado: ${provider}. Use 'ollama', 'gemini' ou 'openai'.`);
          }
          embeddings.push(embedding);
          
          // Log de progresso
          if ((i + 1) % 5 === 0 || i === chunks.length - 1) {
            console.log(`üìä Progresso embeddings: ${i + 1}/${chunks.length}`);
          }
        } catch (error) {
          console.error(`‚ùå Erro ao gerar embedding para chunk ${i}:`, error);
          throw error;
        }
      }

      // Transa√ß√£o para inserir/atualizar documento e chunks
      await session.executeWrite(async (tx) => {
        // Inserir/atualizar documento
        await tx.run(`
          MERGE (d:Document {id: $documentId})
          SET d.name = $name,
              d.hash = $hash,
              d.content = $content,
              d.size = $size,
              d.uploadedAt = $uploadedAt,
              d.processedSecurely = $processedSecurely,
              d.metadata = $metadata
        `, {
          documentId,
          name: document.name,
          hash: documentHash,
          content: document.content,
          size: document.content.length,
          uploadedAt: new Date().toISOString(),
          processedSecurely: true,
          metadata: JSON.stringify(document.metadata)
        });

        // Inserir chunks
        for (let i = 0; i < chunks.length; i++) {
          const chunkId = `${documentId}_chunk_${i}`;
          
          await tx.run(`
            CREATE (c:Chunk {
              id: $chunkId,
              documentId: $documentId,
              content: $content,
              index: $index,
              size: $size,
              embedding: $embedding,
              metadata: $metadata
            })
          `, {
            chunkId,
            documentId,
            content: chunks[i].pageContent,
            index: i,
            size: chunks[i].pageContent.length,
            embedding: embeddings[i],
            metadata: JSON.stringify({
              ...document.metadata,
              chunkIndex: i,
              source: 'memory_upload',
              embeddingModel: 'nomic-embed-text:latest'
            })
          });

          // Criar relacionamento
          await tx.run(`
            MATCH (d:Document {id: $documentId})
            MATCH (c:Chunk {id: $chunkId})
            MERGE (d)-[:CONTAINS]->(c)
          `, { documentId, chunkId });
        }
      });

      const action = shouldUpdate ? 'atualizado' : 'criado';
      console.log(`‚úÖ Documento ${action} com ${provider}: ${document.name} (${chunks.length} chunks)`);
      
    } catch (error: any) {
      console.error(`‚ùå Erro ao processar documento: ${document.name}`, error);
      throw error;
    } finally {
      await session.close();
    }
  }

        async search(query: string, limit: number = 8, modelConfig?: any): Promise<Neo4jSearchResult[]> {
          // Usar estrutura unificada (sem labels espec√≠ficos de provedor)
          const session = this.driver.session();
    
    try {
      // Detectar provider dispon√≠vel automaticamente
      const requestedProvider = modelConfig?.embeddingProvider;
      let provider = await this.getBestAvailableEmbeddingProvider(requestedProvider);
      const baseProvider = provider;
      console.log(`üîç Gerando embedding (com fallback) iniciando por ${provider} para query: "${query.substring(0, 50)}..."`);
      
      let queryEmbedding: number[];
      
      // Verificar cache primeiro
      const cacheKey = `${baseProvider}:${query}`;
      if (this.embeddingCache.has(cacheKey)) {
        console.log(`‚ö° Cache hit para embedding: "${query.substring(0, 30)}..."`);
        queryEmbedding = this.embeddingCache.get(cacheKey)!;
      } else {
        console.log(`üîÑ Cache miss, gerando novo embedding...`);
        
        if (provider === 'ollama') {
          // Se Ollama est√° marcado indispon√≠vel ou ping falhar, usar Gemini direto
          const canTry = !this.ollamaUnavailableUntil || Date.now() > this.ollamaUnavailableUntil;
          const reachable = canTry ? await this.pingOllama(800) : false;
          if (!reachable) {
            console.warn('‚ö†Ô∏è Ollama indispon√≠vel, utilizando Gemini diretamente para esta query');
            provider = 'gemini';
          }
        }

        if (provider === 'ollama') {
          // Usar Ollama para embeddings
          console.log(`üîó Usando Ollama para embedding da query`);
          try {
            const response = await fetch(`${process.env.OLLAMA_BASE_URL ?? ''}/api/embeddings`, {
              method: 'POST',
              headers: { 'Content-Type': 'application/json' },
              body: JSON.stringify({
                model: this.getEmbeddingModel(modelConfig, 'ollama'),
                prompt: query
              })
            });
            
            if (response.ok) {
              const data = await response.json() as { embedding: number[] };
              queryEmbedding = data.embedding;
              
              // Armazenar no cache (limitar tamanho do cache)
              if (this.embeddingCache.size < 100) {
                this.embeddingCache.set(cacheKey, queryEmbedding);
                console.log(`üíæ Embedding armazenado no cache (${this.embeddingCache.size}/100)`);
              }
            } else {
              throw new Error(`Ollama embedding failed: ${response.statusText}`);
            }
          } catch (error) {
            console.warn(`‚ö†Ô∏è Ollama embedding falhou, tentando fallback...`);
            this.ollamaUnavailableUntil = Date.now() + 5 * 60 * 1000; // 5 min cooldown
            // FALLBACK: Gemini somente
            try {
              console.log(`‚Ü™Ô∏è Fallback: Gemini embedding`);
              const { GoogleGenerativeAI } = await import('@google/generative-ai');
              const genAI = new GoogleGenerativeAI(process.env.GEMINI_API_KEY || '');
              const embeddingModel = genAI.getGenerativeModel({ model: this.getEmbeddingModel(modelConfig, 'gemini') });
              const result = await embeddingModel.embedContent(query);
              queryEmbedding = result.embedding.values;
              if (this.embeddingCache.size < 100) {
                this.embeddingCache.set(cacheKey, queryEmbedding);
                console.log(`üíæ Embedding armazenado no cache (${this.embeddingCache.size}/100)`);
              }
            } catch (gerr) {
              // sem OpenAI por padr√£o para evitar terceiros; propagar erro para escolha do outro caminho
              throw gerr;
            }
          }
        } else if (provider === 'gemini') {
          // Usar Gemini para embeddings
          console.log(`üîó Usando Gemini para embedding da query`);
          try {
            const { GoogleGenerativeAI } = await import('@google/generative-ai');
            const genAI = new GoogleGenerativeAI(process.env.GEMINI_API_KEY || '');
            const embeddingModel = genAI.getGenerativeModel({ 
              model: this.getEmbeddingModel(modelConfig, 'gemini')
            });
            
            const result = await embeddingModel.embedContent(query);
            queryEmbedding = result.embedding.values;
            
            // Armazenar no cache (limitar tamanho do cache)
            if (this.embeddingCache.size < 100) {
              this.embeddingCache.set(cacheKey, queryEmbedding);
              console.log(`üíæ Embedding armazenado no cache (${this.embeddingCache.size}/100)`);
            }
          } catch (error) {
            console.warn(`‚ö†Ô∏è Erro com Gemini embedding, tentando fallback para Ollama...`, error);
            // Fallback para Ollama
            const response = await fetch(`${process.env.OLLAMA_BASE_URL ?? ''}/api/embeddings`, {
              method: 'POST',
              headers: { 'Content-Type': 'application/json' },
              body: JSON.stringify({
                model: this.getEmbeddingModel(modelConfig, 'ollama'),
                prompt: query
              })
            });
            if (!response.ok) throw new Error(`Ollama embedding failed: ${response.statusText}`);
            const data = await response.json() as { embedding: number[] };
            queryEmbedding = data.embedding;
            if (this.embeddingCache.size < 100) {
              this.embeddingCache.set(cacheKey, queryEmbedding);
              console.log(`üíæ Embedding armazenado no cache (${this.embeddingCache.size}/100)`);
            }
          }
        } else if (provider === 'openai' || provider === 'openrouter') {
          // Usar OpenAI para embeddings (OpenRouter n√£o suporta embeddings)
          console.log(`üîó Usando OpenAI para embedding da query`);
          const apiKey = process.env.OPENAI_API_KEY || process.env.OPENROUTER_API_KEY;
          
          if (!apiKey) {
            throw new Error('OPENAI_API_KEY ou OPENROUTER_API_KEY n√£o configurada');
          }
          
          try {
            const response = await fetch('https://api.openai.com/v1/embeddings', {
              method: 'POST',
              headers: {
                'Authorization': `Bearer ${apiKey}`,
                'Content-Type': 'application/json'
              },
              body: JSON.stringify({
                model: this.getEmbeddingModel(modelConfig, provider),
                input: query
              })
            });

            if (response.ok) {
              const data = await response.json() as { data: Array<{ embedding: number[] }> };
              queryEmbedding = data.data[0].embedding;
              
              // Armazenar no cache (limitar tamanho do cache)
              if (this.embeddingCache.size < 100) {
                this.embeddingCache.set(cacheKey, queryEmbedding);
                console.log(`üíæ Embedding armazenado no cache (${this.embeddingCache.size}/100)`);
              }
            } else {
              const errorText = await response.text();
              console.error(`‚ùå Erro na resposta da OpenAI:`, errorText);
              throw new Error(`OpenAI embedding failed: ${response.statusText}`);
            }
          } catch (error) {
            console.warn(`‚ö†Ô∏è Erro com OpenAI embedding:`, error);
            throw new Error(`Falha ao gerar embedding com OpenAI: ${error instanceof Error ? error.message : 'Unknown error'}`);
          }
        } else {
          throw new Error(`Provider de embedding n√£o suportado: ${provider}. Use 'ollama', 'gemini' ou 'openai'.`);
        }
      }
      
      // Tentar busca vetorial primeiro
      try {
        const result = await session.run(`
          CALL db.index.vector.queryNodes('chunk_embeddings', $k, $queryEmbedding)
          YIELD node AS chunk, score
          MATCH (d:Document)-[:CONTAINS]->(chunk)
          RETURN chunk, d AS document, score
          ORDER BY score DESC
          LIMIT $limit
        `, {
          k: limit,
          queryEmbedding,
          limit: neo4j.int(limit)
        });

        console.log(`‚úÖ Busca vetorial encontrou ${result.records.length} resultados`);
        return this.parseSearchResults(result);
        
      } catch (vectorError: any) {
        console.warn("‚ö†Ô∏è √çndice vetorial n√£o dispon√≠vel, usando busca por similaridade manual:", vectorError.message);
        
        // Fallback: busca manual por similaridade
        return await this.manualSimilaritySearch(queryEmbedding, limit, session);
      }
      
    } catch (error: any) {
      console.error("‚ùå Erro na busca sem√¢ntica:", error);
      throw error;
    } finally {
      await session.close();
    }
  }

  private async manualSimilaritySearch(queryEmbedding: number[], limit: number, session: any): Promise<Neo4jSearchResult[]> {
    // Buscar todos os chunks e calcular similaridade manualmente
    const result = await session.run(`
      MATCH (d:Document)-[:CONTAINS]->(c:Chunk)
      RETURN c AS chunk, d AS document
      LIMIT 100
    `);

    const results: Neo4jSearchResult[] = [];
    
    for (const record of result.records) {
      const chunkNode = record.get('chunk') as Node;
      const docNode = record.get('document') as Node;
      
      const chunkEmbedding = chunkNode.properties.embedding as number[];
      const score = this.cosineSimilarity(queryEmbedding, chunkEmbedding);

      const chunk: Neo4jChunk = {
        id: chunkNode.properties.id as string,
        documentId: chunkNode.properties.documentId as string,
        content: chunkNode.properties.content as string,
        index: this.safeToNumber(chunkNode.properties.index),
        size: this.safeToNumber(chunkNode.properties.size),
        embedding: chunkEmbedding,
        metadata: JSON.parse(chunkNode.properties.metadata as string)
      };

      const document: Neo4jDocument = {
        id: docNode.properties.id as string,
        name: docNode.properties.name as string,
        hash: docNode.properties.hash as string,
        content: docNode.properties.content as string,
        size: this.safeToNumber(docNode.properties.size),
        uploadedAt: docNode.properties.uploadedAt as string,
        processedSecurely: docNode.properties.processedSecurely as boolean,
        metadata: JSON.parse(docNode.properties.metadata as string)
      };

      results.push({ chunk, document, score });
    }

    // Ordenar por score e limitar
    return results
      .sort((a, b) => b.score - a.score)
      .slice(0, limit);
  }

  private cosineSimilarity(a: number[], b: number[]): number {
    const dotProduct = a.reduce((sum, ai, i) => sum + ai * b[i], 0);
    const normA = Math.sqrt(a.reduce((sum, ai) => sum + ai * ai, 0));
    const normB = Math.sqrt(b.reduce((sum, bi) => sum + bi * bi, 0));
    return dotProduct / (normA * normB);
  }

  private safeToNumber(value: any): number {
    if (typeof value === 'number') {
      return value;
    }
    if (value && typeof value.toNumber === 'function') {
      return value.toNumber();
    }
    if (value && typeof value.low === 'number') {
      return value.low; // Neo4j Integer
    }
    return parseInt(String(value)) || 0;
  }

  private parseSearchResults(result: any): Neo4jSearchResult[] {
    const results: Neo4jSearchResult[] = [];
    
    for (const record of result.records) {
      const chunkNode = record.get('chunk') as Node;
      const docNode = record.get('document') as Node;
      const score = record.get('score') as number;

      const chunk: Neo4jChunk = {
        id: chunkNode.properties.id as string,
        documentId: chunkNode.properties.documentId as string,
        content: chunkNode.properties.content as string,
        index: this.safeToNumber(chunkNode.properties.index),
        size: this.safeToNumber(chunkNode.properties.size),
        embedding: chunkNode.properties.embedding as number[],
        metadata: JSON.parse(chunkNode.properties.metadata as string)
      };

      const document: Neo4jDocument = {
        id: docNode.properties.id as string,
        name: docNode.properties.name as string,
        hash: docNode.properties.hash as string,
        content: docNode.properties.content as string,
        size: this.safeToNumber(docNode.properties.size),
        uploadedAt: docNode.properties.uploadedAt as string,
        processedSecurely: docNode.properties.processedSecurely as boolean,
        metadata: JSON.parse(docNode.properties.metadata as string)
      };

      results.push({ chunk, document, score });
    }

    return results;
  }

  async hybridSearch(query: string, limit: number = 8, modelConfig?: any): Promise<Neo4jSearchResult[]> {
    // Usar apenas busca Neo4j (sem h√≠brido)
    return await this.search(query, limit, modelConfig);
  }

  async verificarCache(): Promise<boolean> {
    const session = this.driver.session();
    
    try {
      const result = await session.run(`
        MATCH (d:Document) 
        RETURN count(d) AS documentCount
      `);
      
      const count = (result.records[0]?.get('documentCount') as Integer)?.toNumber() || 0;
      return count > 0;
      
    } catch (error) {
      return false;
    } finally {
      await session.close();
    }
  }

  async obterEstatisticas(): Promise<{ totalChunks: number; totalDocumentos: number }> {
    const session = this.driver.session();
    
    try {
      // Contar todos os documentos e chunks (sem labels espec√≠ficos de provedor)
      const result = await session.run(`
        MATCH (d:Document)
        OPTIONAL MATCH (d)-[:CONTAINS]->(c:Chunk)
        RETURN count(DISTINCT d) AS totalDocumentos, count(c) AS totalChunks
      `);
      
      const record = result.records[0];
      const totalDocumentos = (record?.get('totalDocumentos') as Integer)?.toNumber() || 0;
      const totalChunks = (record?.get('totalChunks') as Integer)?.toNumber() || 0;
      
      console.log(`üìä Total: ${totalDocumentos} documentos, ${totalChunks} chunks`);
      
      return { totalChunks, totalDocumentos };
      
    } finally {
      await session.close();
    }
  }

  async limparCache(): Promise<void> {
    const session = this.driver.session();
    
    try {
      await session.run(`
        MATCH (d:Document)
        DETACH DELETE d
      `);
      
      await session.run(`
        MATCH (c:Chunk)
        DELETE c
      `);
      
      console.log("üóëÔ∏è Cache Neo4j limpo");
      
    } finally {
      await session.close();
    }
  }

  async close(): Promise<void> {
    await this.driver.close();
  }
}
