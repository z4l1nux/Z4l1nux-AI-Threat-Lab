# AI TRiSM Framework (Gartner)

## AI Trust, Risk and Security Management

**Fonte**: Gartner - AI TRiSM Framework  
**Vers√£o**: 2024-2025

---

## Vis√£o Geral

**AI TRiSM** √© um framework do Gartner para gerenciar confian√ßa, risco e seguran√ßa em sistemas de Intelig√™ncia Artificial, garantindo que modelos de IA sejam:

- ‚úÖ **Confi√°veis** (Trustworthy)
- ‚úÖ **Seguros** (Secure)
- ‚úÖ **Privados** (Privacy-preserving)
- ‚úÖ **√âticos** (Ethical)
- ‚úÖ **Transparentes** (Transparent)

---

## Os 4 Pilares do AI TRiSM

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    AI TRiSM Framework                    ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  Pilar 1:     ‚îÇ  Pilar 2:     ‚îÇ  Pilar 3:     ‚îÇ  Pilar 4:‚îÇ
‚îÇ Explicabilidade‚îÇ   ModelOps    ‚îÇ  Privacidade  ‚îÇ Seguran√ßa‚îÇ
‚îÇ e Monitoramento‚îÇ               ‚îÇ   de IA       ‚îÇ de IA    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üîç Pilar 1: Explicabilidade e Monitoramento

### Objetivo
Tornar decis√µes de IA compreens√≠veis, audit√°veis e monitor√°veis.

### Componentes

#### 1.1 Explicabilidade (XAI - Explainable AI)

**T√©cnicas**:
- **SHAP (SHapley Additive exPlanations)**
  - Valores de contribui√ß√£o de cada feature
  - Model-agnostic
  - Ferramenta: `shap` Python library

- **LIME (Local Interpretable Model-agnostic Explanations)**
  - Explica√ß√µes locais por instance
  - Aproxima√ß√£o linear local
  - Ferramenta: `lime` Python library

- **Attention Visualizations**
  - Para transformers/LLMs
  - Heatmaps de aten√ß√£o
  - Ferramenta: BertViz, Captum

- **Feature Importance**
  - Permutation importance
  - Feature ablation
  - Partial dependence plots

**Implementa√ß√£o**:
```python
import shap

# Explicar predi√ß√£o de LLM
explainer = shap.Explainer(model)
shap_values = explainer(input_text)
shap.plots.text(shap_values)
```

#### 1.2 Transpar√™ncia

**Model Cards**:
- Documenta√ß√£o padronizada de modelos
- Campos: Uso intencionado, limita√ß√µes, performance, vieses
- Template: Google Model Cards, Hugging Face

**System Cards** (OpenAI):
- Documenta√ß√£o de sistema completo
- Inclui: Arquitetura, dados, safety measures

**Datasheets for Datasets**:
- Documenta√ß√£o de datasets
- Origem, coleta, composi√ß√£o, uso recomendado

#### 1.3 Monitoramento Cont√≠nuo

**M√©tricas a Monitorar**:
- **Data Drift**: Mudan√ßa na distribui√ß√£o de entrada
- **Concept Drift**: Mudan√ßa no conceito (X ‚Üí Y)
- **Performance Degradation**: Queda de accuracy/F1
- **Bias Metrics**: Disparate impact, demographic parity
- **Latency**: Tempo de resposta
- **Error Rate**: Taxa de erros em produ√ß√£o

**Ferramentas**:
- **Evidently AI**: Monitoring e dashboards
- **WhyLabs**: Observability para ML
- **Fiddler AI**: Model monitoring
- **Arize AI**: ML observability

**Alertas**:
- Drift detection (statistical tests)
- Performance threshold breach
- Anomaly detection

### Categoria STRIDE
- **Repudiation**: Auditoria de decis√µes
- **Information Disclosure**: Transpar√™ncia controlada

### Mapeamento OWASP LLM
- **LLM06**: Sensitive Information Disclosure (monitoring)
- **LLM09**: Overreliance (explicabilidade)

---

## ‚öôÔ∏è Pilar 2: ModelOps (MLOps)

### Objetivo
Operacionalizar o ciclo de vida completo de modelos de IA/ML.

### Componentes

#### 2.1 Ciclo de Vida do Modelo

```
Desenvolvimento ‚Üí Treinamento ‚Üí Valida√ß√£o ‚Üí Deployment ‚Üí 
Monitoramento ‚Üí Retreinamento ‚Üí Decommissioning
```

#### 2.2 CI/CD para ML

**Pipeline Automatizado**:
1. **Data Validation**: Schema validation, drift detection
2. **Model Training**: Automated retraining on new data
3. **Model Testing**: Unit tests, integration tests
4. **Model Validation**: Performance thresholds
5. **Deployment**: Canary, blue-green, A/B testing
6. **Monitoring**: Real-time performance tracking

**Ferramentas**:
- **MLflow**: Experiment tracking, model registry
- **Kubeflow**: ML pipelines em Kubernetes
- **Weights & Biases**: Experiment management
- **DVC (Data Version Control)**: Versionamento de dados
- **ClearML**: MLOps platform

#### 2.3 Versionamento

**Versionar**:
- C√≥digo (Git)
- Dados (DVC, LakeFS)
- Modelos (MLflow Model Registry)
- Configs (YAML, Hydra)
- Experimentos (W&B, MLflow)

**Reprodutibilidade**:
- Seeds fixos
- Environment pinning (requirements.txt, Poetry)
- Docker containers
- Documenta√ß√£o completa

#### 2.4 Governan√ßa de Modelos

**Model Registry**:
- Cat√°logo centralizado de modelos
- Status: Development, Staging, Production, Archived
- Metadata: Owner, version, performance, approvals

**Approval Workflow**:
- Revis√£o por stakeholders
- Security scanning
- Bias auditing
- Performance validation

**Rollback Capability**:
- Reverter para vers√£o anterior
- Hot-swap de modelos
- Zero-downtime deployment

#### 2.5 Feature Store

**Benef√≠cios**:
- Reuso de features entre modelos
- Consist√™ncia treino/serving
- Low-latency serving

**Ferramentas**:
- **Feast**: Open-source feature store
- **Tecton**: Enterprise feature platform
- **AWS SageMaker Feature Store**
- **Databricks Feature Store**

### Categoria STRIDE
- **Tampering**: Versionamento previne adultera√ß√£o
- **Repudiation**: Audit trail completo

### Mapeamento OWASP LLM
- **LLM03**: Training Data Poisoning (valida√ß√£o)
- **LLM05**: Supply Chain Vulnerabilities (governance)

---

## üîí Pilar 3: Privacidade de IA

### Objetivo
Proteger privacidade dos dados usados para treinar e servir modelos de IA.

### Componentes

#### 3.1 Differential Privacy (DP)

**Conceito**:
- Adicionar ru√≠do aos dados/modelo
- Garantir que remo√ß√£o de um indiv√≠duo n√£o afeta output
- M√©trica: Œµ (epsilon) - privacy budget

**T√©cnicas**:
- **DP-SGD**: Differentially Private Stochastic Gradient Descent
- **PATE**: Private Aggregation of Teacher Ensembles
- **Local DP**: Ru√≠do adicionado no cliente

**Implementa√ß√£o**:
```python
from opacus import PrivacyEngine

# PyTorch com Differential Privacy
privacy_engine = PrivacyEngine()
model, optimizer, dataloader = privacy_engine.make_private(
    module=model,
    optimizer=optimizer,
    data_loader=dataloader,
    noise_multiplier=1.0,
    max_grad_norm=1.0
)
```

**Ferramentas**:
- **Opacus** (PyTorch): Differential privacy library
- **TensorFlow Privacy**: DP para TensorFlow
- **Google DP Library**: C++ e Go

#### 3.2 Federated Learning

**Conceito**:
- Treinar modelo sem centralizar dados
- Modelo vai at√© os dados (edge devices)
- Apenas gradientes s√£o agregados

**Arquitetura**:
```
[Device 1] ‚îÄ‚îê
[Device 2] ‚îÄ‚îº‚Üí [Aggregation Server] ‚Üí [Global Model]
[Device 3] ‚îÄ‚îò
```

**Ferramentas**:
- **TensorFlow Federated**: FL framework
- **PySyft**: Privacy-preserving ML
- **Flower**: Federated learning framework
- **FedML**: Cross-silo and cross-device FL

#### 3.3 Homomorphic Encryption (HE)

**Conceito**:
- Computa√ß√£o em dados criptografados
- Resultado descriptografado √© correto
- Permite ML inference sem ver dados

**Tipos**:
- **Partial HE**: Apenas uma opera√ß√£o (+ ou √ó)
- **Somewhat HE**: N√∫mero limitado de opera√ß√µes
- **Fully HE**: Opera√ß√µes arbitr√°rias

**Ferramentas**:
- **SEAL** (Microsoft): FHE library
- **PALISADE**: Lattice crypto library
- **TenSEAL**: HE sobre tensores

#### 3.4 Secure Multi-Party Computation (SMPC)

**Conceito**:
- M√∫ltiplas partes computam fun√ß√£o sem revelar inputs
- Cada parte tem "share" secreto
- Resultado revelado apenas no final

**Use Cases**:
- Treinar modelo em dados de m√∫ltiplas organiza√ß√µes
- Inference em dados privados

**Ferramentas**:
- **CrypTen** (Facebook): MPC para PyTorch
- **TF Encrypted**: MPC para TensorFlow

#### 3.5 Data Minimization

**Princ√≠pios**:
- Coletar apenas o necess√°rio
- Remover PII dos datasets
- Anonymization/Pseudonymization

**T√©cnicas**:
- **K-anonymity**: Grupos de k indiv√≠duos indistingu√≠veis
- **L-diversity**: Diversidade de valores sens√≠veis
- **T-closeness**: Distribui√ß√£o de valores sens√≠veis

**Ferramentas**:
- **ARX Data Anonymization Tool**
- **Microsoft Presidio**: PII detection e redaction

### Categoria STRIDE
- **Information Disclosure**: Prote√ß√£o de dados sens√≠veis

### Mapeamento OWASP LLM
- **LLM06**: Sensitive Information Disclosure (privacy)
- **LLM03**: Training Data Poisoning (DP ajuda prevenir)

---

## üõ°Ô∏è Pilar 4: Seguran√ßa de Aplica√ß√µes de IA

### Objetivo
Proteger modelos de IA contra ataques adversariais e garantir robustez.

### Componentes

#### 4.1 Adversarial Robustness

**Tipos de Ataques**:
- **Evasion Attacks**: Enganar modelo em inference
- **Poisoning Attacks**: Contaminar dados de treino
- **Model Extraction**: Roubar modelo via queries
- **Model Inversion**: Reconstruir dados de treino

**Adversarial Examples**:
```python
# FGSM Attack
epsilon = 0.03
perturbation = epsilon * sign(gradient)
adversarial_input = original_input + perturbation
```

#### 4.2 Defesas Adversariais

**Adversarial Training**:
- Treinar modelo com adversarial examples
- Aumenta robustez mas reduz accuracy

**Defensive Distillation**:
- Treinar modelo com soft labels
- Dificulta gradient-based attacks

**Input Sanitization**:
- Detectar e remover perturba√ß√µes
- T√©cnicas: Denoising, compression

**Certified Defenses**:
- Garantias matem√°ticas de robustez
- Randomized smoothing

**Ferramentas**:
- **Adversarial Robustness Toolbox (ART)**: IBM
- **CleverHans**: Google
- **Foolbox**: University of T√ºbingen
- **TextAttack**: NLP adversarial attacks

#### 4.3 Input Validation para LLMs

**Valida√ß√µes**:
- Length limits
- Character filtering
- Prompt injection detection
- Malicious pattern matching

**Prompt Firewalls**:
- **Rebuff.ai**: Prompt injection detector
- **NeMo Guardrails**: NVIDIA
- **LangKit**: WhyLabs

#### 4.4 Model Hardening

**T√©cnicas**:
- **Quantization**: Reduz precis√£o (int8)
- **Pruning**: Remove neur√¥nios desnecess√°rios
- **Distillation**: Modelo menor treinado com modelo grande
- **Ensemble Methods**: M√∫ltiplos modelos votam

#### 4.5 Secure Inference

**Isolamento**:
- Containers (Docker, Kubernetes)
- Sandboxing (gVisor, Firecracker)
- Network isolation

**Resource Limits**:
- CPU/GPU quotas
- Memory limits
- Timeout enforcement

**Monitoring**:
- Anomaly detection
- Intrusion detection
- Rate limiting

### Categoria STRIDE
- **Tampering**: Adversarial attacks
- **Denial of Service**: Resource exhaustion
- **Elevation of Privilege**: Model exploitation

### Mapeamento OWASP LLM
- **LLM01**: Prompt Injection (input validation)
- **LLM04**: Model DoS (resource limits)
- **LLM10**: Model Theft (secure inference)

---

## üîÑ Integra√ß√£o dos 4 Pilares

### Fluxo Completo

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 1. Design com Explicabilidade                            ‚îÇ
‚îÇ    ‚Üí Model Cards, requisitos de transpar√™ncia            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
               ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 2. Desenvolvimento com Privacidade                       ‚îÇ
‚îÇ    ‚Üí DP, Federated Learning, data minimization           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
               ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 3. Deployment com ModelOps                               ‚îÇ
‚îÇ    ‚Üí CI/CD, versioning, A/B testing                      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
               ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 4. Opera√ß√£o com Seguran√ßa                                ‚îÇ
‚îÇ    ‚Üí Monitoring, hardening, adversarial defense          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
               ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 5. Feedback Loop para Monitoramento                      ‚îÇ
‚îÇ    ‚Üí Drift detection, performance tracking, retrein      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üìä M√©tricas de Sucesso

### Por Pilar

| Pilar | M√©trica | Target | Ferramenta |
|-------|---------|--------|------------|
| **Explicabilidade** | % decis√µes explic√°veis | > 90% | SHAP, LIME |
| **Explicabilidade** | Mean explanation time | < 2s | Captum |
| **ModelOps** | Deployment frequency | > 1x/semana | MLflow |
| **ModelOps** | MTTR (Mean Time to Restore) | < 1h | Kubeflow |
| **Privacidade** | Privacy budget (Œµ) | < 10 | Opacus |
| **Privacidade** | PII leakage rate | 0% | Presidio |
| **Seguran√ßa** | Adversarial accuracy | > 80% | ART |
| **Seguran√ßa** | Time to detect attack | < 5min | Evidently |

---

## üõ†Ô∏è Ferramentas Recomendadas

### Stack Completo

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Explicabilidade: SHAP, LIME, Captum                     ‚îÇ
‚îÇ ModelOps: MLflow, Kubeflow, Weights & Biases            ‚îÇ
‚îÇ Privacidade: Opacus, TF Privacy, PySyft                 ‚îÇ
‚îÇ Seguran√ßa: ART, Rebuff.ai, NeMo Guardrails              ‚îÇ
‚îÇ Monitoring: Evidently AI, WhyLabs, Fiddler AI           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üìö Refer√™ncias

- Gartner: AI TRiSM Framework
- NIST AI Risk Management Framework
- ISO/IEC 42001: AI Management System
- IEEE 7000 series: AI Standards

---

**Vers√£o**: 2025.1  
**√öltima Atualiza√ß√£o**: Outubro 2025  
**Licen√ßa**: CC BY-SA 4.0

