#!/bin/bash

# Script para configurar Ollama automaticamente baseado nos modelos disponÃ­veis
# RTX 4050 (6GB VRAM) - ConfiguraÃ§Ãµes otimizadas

echo "ðŸš€ Configurando Ollama para RTX 4050 (6GB VRAM)..."

# Verificar se .env.local existe
if [ ! -f ".env.local" ]; then
    echo "ðŸ“ Criando .env.local..."
    touch .env.local
fi

# Backup do .env.local existente
if [ -f ".env.local" ]; then
    cp .env.local .env.local.backup
    echo "ðŸ’¾ Backup criado: .env.local.backup"
fi

# Adicionar configuraÃ§Ãµes Ollama ao .env.local
echo ""
echo "ðŸ”§ Adicionando configuraÃ§Ãµes Ollama ao .env.local..."

# Verificar se jÃ¡ existem configuraÃ§Ãµes Ollama
if grep -q "OLLAMA_DEFAULT_CONTEXT_SIZE" .env.local; then
    echo "âš ï¸  ConfiguraÃ§Ãµes Ollama jÃ¡ existem no .env.local"
    echo "   Removendo configuraÃ§Ãµes antigas..."
    # Remover linhas antigas do Ollama
    sed -i '/^# OLLAMA/,/^OLLAMA_/d' .env.local
fi

# Adicionar configuraÃ§Ãµes otimizadas
cat >> .env.local << 'EOF'

# ===========================================
# OLLAMA CONFIGURATION (RTX 4050 - 6GB VRAM)
# ===========================================
OLLAMA_BASE_URL=http://172.21.112.1:11434
OLLAMA_TIMEOUT=180000
OLLAMA_MAX_RETRIES=2

# Contexto padrÃ£o para todos os modelos (8k tokens)
OLLAMA_DEFAULT_CONTEXT_SIZE=8192

# ConfiguraÃ§Ãµes especÃ­ficas por modelo (baseado nos seus modelos disponÃ­veis)
# Modelos leves - contexto maior
OLLAMA_CONTEXT_MISTRAL_LATEST=4096
OLLAMA_CONTEXT_LLAMA3_1_LATEST=8192
OLLAMA_CONTEXT_PHI4_MINI_LATEST=4096
OLLAMA_CONTEXT_GRANITE3_3_8B=8192

# Modelos mÃ©dios - contexto moderado
OLLAMA_CONTEXT_QWEN2_5_CODER_7B=4096
OLLAMA_CONTEXT_QWEN3_8B=4096
OLLAMA_CONTEXT_DEEPSEEK_R1_LATEST=6144
OLLAMA_CONTEXT_WHITERABBITNEO_2_5_QWEN_2_5_CODER_7B_LATEST=4096

# CompressÃ£o automÃ¡tica
OLLAMA_AUTO_COMPRESS=true
OLLAMA_COMPRESSION_RATIO=3

# ConfiguraÃ§Ãµes de contexto RAG (sem hardcoding)
OLLAMA_DEFAULT_CONTEXT_LIMIT=1000
OLLAMA_LOCAL_CONTEXT_LIMIT=300
OLLAMA_AI_SYSTEM_CONTEXT_LIMIT=600
OLLAMA_LIMITED_CONTEXT_LIMIT=150

# Modelos com contexto limitado (configurÃ¡vel)
OLLAMA_LIMITED_CONTEXT_PHI4_MINI_LATEST=true
OLLAMA_LIMITED_CONTEXT_MISTRAL_LATEST=true
OLLAMA_LIMITED_CONTEXT_QWEN2_5_CODER_7B=true
OLLAMA_LIMITED_CONTEXT_QWEN3_8B=true

# CompressÃ£o agressiva por modelo (configurÃ¡vel)
OLLAMA_AGGRESSIVE_COMPRESS_PHI4_MINI_LATEST=true
OLLAMA_AGGRESSIVE_COMPRESS_MISTRAL_LATEST=true
OLLAMA_AGGRESSIVE_COMPRESS_QWEN2_5_CODER_7B=true
OLLAMA_AGGRESSIVE_COMPRESS_QWEN3_8B=true
EOF

echo "âœ… ConfiguraÃ§Ãµes adicionadas ao .env.local"
echo ""
echo "ðŸ“‹ ConfiguraÃ§Ãµes aplicadas:"
echo "   - Contexto padrÃ£o: 8192 tokens"
echo "   - Modelos leves: 4096-8192 tokens"
echo "   - Modelos mÃ©dios: 4096-6144 tokens"
echo "   - CompressÃ£o automÃ¡tica: habilitada"
echo "   - CompressÃ£o agressiva: configurÃ¡vel por modelo"
echo "   - Contexto RAG: configurÃ¡vel por tipo de sistema"
echo ""
echo "ðŸŽ¯ PrÃ³ximos passos:"
echo "   1. Reinicie o backend: npm run dev:full"
echo "   2. Teste com um modelo: phi4-mini:latest ou mistral:latest"
echo "   3. Monitore VRAM: nvidia-smi"
echo "   4. Ajuste configuraÃ§Ãµes no .env.local se necessÃ¡rio"
echo ""
echo "ðŸ’¡ Dicas:"
echo "   - Use phi4-mini:latest para melhor contexto (4k tokens)"
echo "   - Use mistral:latest para melhor equilÃ­brio (4k tokens)"
echo "   - Configure OLLAMA_LIMITED_CONTEXT_* para modelos especÃ­ficos"
echo "   - Configure OLLAMA_AGGRESSIVE_COMPRESS_* para compressÃ£o agressiva"
