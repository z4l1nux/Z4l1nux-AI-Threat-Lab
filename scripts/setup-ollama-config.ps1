# Script para configurar Ollama automaticamente baseado nos modelos dispon√≠veis
# RTX 4050 (6GB VRAM) - Configura√ß√µes otimizadas
# PowerShell equivalente ao setup-ollama-config.sh

Write-Host "üöÄ Configurando Ollama para RTX 4050 (6GB VRAM)..." -ForegroundColor Green

# Verificar se .env.local existe
if (-not (Test-Path ".env.local")) {
    Write-Host "üìù Criando .env.local..." -ForegroundColor Cyan
    New-Item -ItemType File -Path ".env.local" -Force | Out-Null
}

# Backup do .env.local existente
if (Test-Path ".env.local") {
    Copy-Item ".env.local" ".env.local.backup" -Force
    Write-Host "üíæ Backup criado: .env.local.backup" -ForegroundColor Green
}

# Verificar se j√° existem configura√ß√µes Ollama
$envContent = Get-Content ".env.local" -Raw -ErrorAction SilentlyContinue
if ($envContent -and $envContent.Contains("OLLAMA_DEFAULT_CONTEXT_SIZE")) {
    Write-Host "‚ö†Ô∏è Configura√ß√µes Ollama j√° existem no .env.local" -ForegroundColor Yellow
    Write-Host "   Removendo configura√ß√µes antigas..." -ForegroundColor Yellow
    
    # Remover linhas antigas do Ollama
    $lines = Get-Content ".env.local" | Where-Object { 
        $_ -notmatch "^# OLLAMA" -and 
        $_ -notmatch "^OLLAMA_" -and 
        $_ -notmatch "^$"
    }
    $lines | Set-Content ".env.local"
}

# Adicionar configura√ß√µes otimizadas
Write-Host "`nüîß Adicionando configura√ß√µes Ollama ao .env.local..." -ForegroundColor Cyan

$ollamaConfig = @"

# ===========================================
# OLLAMA CONFIGURATION (RTX 4050 - 6GB VRAM)
# ===========================================
OLLAMA_BASE_URL=http://172.21.112.1:11434
OLLAMA_TIMEOUT=180000
OLLAMA_MAX_RETRIES=2

# Contexto padr√£o para todos os modelos (8k tokens)
OLLAMA_DEFAULT_CONTEXT_SIZE=8192

# Configura√ß√µes espec√≠ficas por modelo (baseado nos seus modelos dispon√≠veis)
# Modelos leves - contexto maior
OLLAMA_CONTEXT_MISTRAL_LATEST=4096
OLLAMA_CONTEXT_LLAMA3_1_LATEST=8192
OLLAMA_CONTEXT_PHI4_MINI_LATEST=4096
OLLAMA_CONTEXT_GRANITE3_3_8B=8192

# Modelos m√©dios - contexto moderado
OLLAMA_CONTEXT_QWEN2_5_CODER_7B=4096
OLLAMA_CONTEXT_QWEN3_8B=4096
OLLAMA_CONTEXT_DEEPSEEK_R1_LATEST=6144
OLLAMA_CONTEXT_WHITERABBITNEO_2_5_QWEN_2_5_CODER_7B_LATEST=4096

# Compress√£o autom√°tica
OLLAMA_AUTO_COMPRESS=true
OLLAMA_COMPRESSION_RATIO=3

# Configura√ß√µes de contexto RAG (sem hardcoding)
OLLAMA_DEFAULT_CONTEXT_LIMIT=1000
OLLAMA_LOCAL_CONTEXT_LIMIT=300
OLLAMA_AI_SYSTEM_CONTEXT_LIMIT=600
OLLAMA_LIMITED_CONTEXT_LIMIT=150

# Modelos com contexto limitado (configur√°vel)
OLLAMA_LIMITED_CONTEXT_PHI4_MINI_LATEST=true
OLLAMA_LIMITED_CONTEXT_MISTRAL_LATEST=true
OLLAMA_LIMITED_CONTEXT_QWEN2_5_CODER_7B=true
OLLAMA_LIMITED_CONTEXT_QWEN3_8B=true

# Compress√£o agressiva por modelo (configur√°vel)
OLLAMA_AGGRESSIVE_COMPRESS_PHI4_MINI_LATEST=true
OLLAMA_AGGRESSIVE_COMPRESS_MISTRAL_LATEST=true
OLLAMA_AGGRESSIVE_COMPRESS_QWEN2_5_CODER_7B=true
OLLAMA_AGGRESSIVE_COMPRESS_QWEN3_8B=true
"@

Add-Content -Path ".env.local" -Value $ollamaConfig

Write-Host "‚úÖ Configura√ß√µes adicionadas ao .env.local" -ForegroundColor Green
Write-Host ""
Write-Host "üìã Configura√ß√µes aplicadas:" -ForegroundColor Cyan
Write-Host "   - Contexto padr√£o: 8192 tokens" -ForegroundColor White
Write-Host "   - Modelos leves: 4096-8192 tokens" -ForegroundColor White
Write-Host "   - Modelos m√©dios: 4096-6144 tokens" -ForegroundColor White
Write-Host "   - Compress√£o autom√°tica: habilitada" -ForegroundColor White
Write-Host "   - Compress√£o agressiva: configur√°vel por modelo" -ForegroundColor White
Write-Host "   - Contexto RAG: configur√°vel por tipo de sistema" -ForegroundColor White
Write-Host ""
Write-Host "üéØ Pr√≥ximos passos:" -ForegroundColor Cyan
Write-Host "   1. Reinicie o backend: npm run dev:full" -ForegroundColor White
Write-Host "   2. Teste com um modelo: phi4-mini:latest ou mistral:latest" -ForegroundColor White
Write-Host "   3. Monitore VRAM: nvidia-smi" -ForegroundColor White
Write-Host "   4. Ajuste configura√ß√µes no .env.local se necess√°rio" -ForegroundColor White
Write-Host ""
Write-Host "üí° Dicas:" -ForegroundColor Cyan
Write-Host "   - Use phi4-mini:latest para melhor contexto (4k tokens)" -ForegroundColor White
Write-Host "   - Use mistral:latest para melhor equil√≠brio (4k tokens)" -ForegroundColor White
Write-Host "   - Configure OLLAMA_LIMITED_CONTEXT_* para modelos espec√≠ficos" -ForegroundColor White
Write-Host "   - Configure OLLAMA_AGGRESSIVE_COMPRESS_* para compress√£o agressiva" -ForegroundColor White

# Verificar se o projeto est√° configurado
if (Test-Path "package.json") {
    Write-Host "`nüîç Verificando configura√ß√£o do projeto..." -ForegroundColor Cyan
    
    # Verificar se backend est√° configurado
    if (Test-Path "backend/package.json") {
        Write-Host "‚úÖ Backend encontrado" -ForegroundColor Green
    } else {
        Write-Host "‚ö†Ô∏è Backend n√£o encontrado" -ForegroundColor Yellow
    }
    
    # Verificar se frontend est√° configurado
    if (Test-Path "src") {
        Write-Host "‚úÖ Frontend encontrado" -ForegroundColor Green
    } else {
        Write-Host "‚ö†Ô∏è Frontend n√£o encontrado" -ForegroundColor Yellow
    }
    
    Write-Host "`nüöÄ Para iniciar o projeto:" -ForegroundColor Cyan
    Write-Host "   npm run dev:full" -ForegroundColor White
} else {
    Write-Host "`n‚ö†Ô∏è Este n√£o parece ser o diret√≥rio do projeto Threat Modeling Co-Pilot" -ForegroundColor Yellow
    Write-Host "   Execute este script no diret√≥rio raiz do projeto" -ForegroundColor Yellow
}
