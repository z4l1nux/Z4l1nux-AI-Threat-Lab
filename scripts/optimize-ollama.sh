#!/bin/bash

# Script para otimizar Ollama para RTX 4050 (6GB VRAM)
# Configura√ß√µes otimizadas para diferentes modelos

echo "üöÄ Otimizando Ollama para RTX 4050 (6GB VRAM)..."

# Fun√ß√£o para configurar modelo com contexto otimizado
configure_model() {
    local model_name=$1
    local context_size=$2
    local description=$3
    
    echo "üì¶ Configurando $model_name ($description)"
    echo "   Contexto: $context_size tokens"
    
    # Parar modelo se estiver rodando
    ollama stop $model_name 2>/dev/null || true
    
    # Configurar com contexto otimizado
    ollama run $model_name --numa --ctx-size $context_size &
    
    echo "‚úÖ $model_name configurado com sucesso"
    echo ""
}

# Configura√ß√µes otimizadas para RTX 4050
echo "üîß Configura√ß√µes recomendadas para RTX 4050:"
echo ""

# Modelos leves (melhor performance)
configure_model "llama3.1:8b" 8192 "Llama 3.1 8B - Equilibrio performance/mem√≥ria"
configure_model "gemma2:9b" 16384 "Gemma 2 9B - Excelente contexto, mais eficiente"
configure_model "mistral:7b" 8192 "Mistral 7B - Boa performance geral"

# Modelos m√©dios (se tiver VRAM extra)
configure_model "llama3.1:70b" 4096 "Llama 3.1 70B - Modelo maior, contexto reduzido"

echo "üéØ Configura√ß√µes aplicadas!"
echo ""
echo "üí° Dicas de uso:"
echo "   - Use llama3.1:8b para melhor equil√≠brio"
echo "   - Use gemma2:9b para contexto maior (16k tokens)"
echo "   - Use mistral:7b para performance consistente"
echo ""
echo "‚ö†Ô∏è  Monitoramento:"
echo "   - Verifique uso de VRAM: nvidia-smi"
echo "   - Ajuste contexto se necess√°rio"
echo "   - Reinicie Ollama se houver problemas de mem√≥ria"
